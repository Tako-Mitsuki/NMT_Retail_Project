# Auto-generated from Copy_of_NMT_Online_Retail.ipynb
# Module: preprocessing

'''
Review this file: the code was heuristically split from the notebook.
Move functions/classes around as needed.
'''

import pandas as pd
import matplotlib.pyplot as plt
# ƒê·ªçc file CSV v·ªõi encoding kh√°c (v√≠ d·ª•: 'ISO-8859-1')
df = pd.read_csv("OnlineRetail.csv", encoding='ISO-8859-1')

# In k√≠ch th∆∞·ªõc v√† th√¥ng tin d·ªØ li·ªáu
print("K√≠ch th∆∞·ªõc ban ƒë·∫ßu:", df.shape)
print(df.info())


df["InvoiceDate"] = pd.to_datetime(df["InvoiceDate"], errors="coerce")
df["InvoiceDate_day"] = df["InvoiceDate"].dt.day

#.dt.day: cho ph√©p truy c·∫≠p thu·ªôc t√≠nh ng√†y thu·ªôc t√≠nh datetime.

df["InvoiceDate_hour"] = df["InvoiceDate"].dt.hour
df["InvoiceDate_month"] = df["InvoiceDate"].dt.month


df["IsCancelled"] = df["Quantity"] < 0
before_rows = len(df)
df = df[df["Quantity"] > 0]   # b·ªè c√°c d√≤ng c√≥ Quantity √¢m
removed_cancelled = before_rows - len(df)


missing_before = df["CustomerID"].isna().sum()
df["CustomerID"] = df["CustomerID"].fillna("Guest")
missing_after = df["CustomerID"].isna().sum()


df["Description"] = (
    df["Description"]
    .astype(str)
    .str.lower()
    .str.strip()
    .str.replace(r"\s+", " ", regex=True)
)


before_rows2 = len(df)
df = df[(df["Quantity"] < 10000) & (df["UnitPrice"] > 0) & (df["UnitPrice"] < 10000)]
removed_outliers = before_rows2 - len(df)

# Quantity >= 10000: Giao d·ªãch kh√¥ng h·ª£p l√Ω b√°n l·∫ª. L·ªói nh·∫≠p d·ªØ li·ªáu
# UnitPrice <=0: ƒë∆°n b·ªã tr·∫£ h√†ng, ƒë∆°n b·ªã h·ªßy v√† d·ª± li·ªáu sai. Kh√¥ng ƒë·∫°i di·ªán cho h√†nh vi mua h√†ng n√™n c·∫ßn lo·∫°i b·ªè.
# UnitPrice >= 10000: Kh√¥ng th·ª±c t·∫ø v·ªõi s·∫£n ph·∫©m b√°n l·∫ª th√¥ng th∆∞·ªùng.


print("K√≠ch th∆∞·ªõc cu·ªëi:", df.shape)
print(f"S·ªë d√≤ng b·ªã lo·∫°i b·ªè do Quantity √¢m: {removed_cancelled}")
print(f"S·ªë d√≤ng b·ªã lo·∫°i b·ªè do ngo·∫°i l·ªá (UnitPrice/Quantity): {removed_outliers}")
print(f"S·ªë CustomerID b·ªã thi·∫øu tr∆∞·ªõc khi x·ª≠ l√Ω: {missing_before}, sau khi x·ª≠ l√Ω: {missing_after}")
print("S·ªë l∆∞·ª£ng giao d·ªãch:", df["InvoiceNo"].nunique())
print("S·ªë l∆∞·ª£ng kh√°ch h√†ng:", df["CustomerID"].nunique())
print("S·ªë l∆∞·ª£ng s·∫£n ph·∫©m:", df["Description"].nunique())


# T·∫°o c·ªôt doanh thu
df["Revenue"] = df["Quantity"] * df["UnitPrice"]

# Doanh thu theo ng√†y
revenue_by_day = df.groupby(df["InvoiceDate"].dt.date)["Revenue"].sum()

# Doanh thu theo th√°ng
revenue_by_month = df.groupby(df["InvoiceDate"].dt.to_period("M"))["Revenue"].sum()

print("üîπ Doanh thu theo ng√†y (5 d√≤ng ƒë·∫ßu):")
print(revenue_by_day.head())
print("\nüîπ Doanh thu theo th√°ng:")
print(revenue_by_month.head())

# V·∫Ω
# https://pandas.pydata.org/docs/user_guide/visualization.html#time-series-plotting
revenue_by_day.plot(figsize=(12,6), title="Doanh thu theo ng√†y")
plt.ylabel("Doanh thu")
plt.show()

revenue_by_month.plot(kind="bar", figsize=(12,6), title="Doanh thu theo th√°ng")
plt.ylabel("Doanh thu")
plt.show()


top10_quantity = df.groupby("Description")["Quantity"].sum().sort_values(ascending=False).head(10)
top10_revenue = df.groupby("Description")["Revenue"].sum().sort_values(ascending=False).head(10)

# grouby("Description"): gom nh√≥m d·ªØ li·ªáu theo Description v√† m·ªói nh√≥m = 1 lo·∫°i s·∫£n ph·∫©m.
# ["Quantity"].sum(): t√≠nh t·ªïng s·ªë l∆∞·ª£ng b√°n ra ·ª©ng v·ªõi m·ªói s·∫£n ph·∫©m.
# ["Revenue"].sum(): t√≠nh t·ªïng doanh thu d·ª±a tr√™n m·ªói s·∫£n ph·∫©m.
# sort_values(ascending=False): s·∫Øp x·∫øp thep gi√° tr·ªã gi·∫£m d·∫ßn (∆∞u ti√™n c√°c s·∫£n ph·∫©m b√°n nhi·ªÅu nh·∫•t).

print(" Top 10 s·∫£n ph·∫©m theo s·ªë l∆∞·ª£ng:")
print(top10_quantity)

print("\n Top 10 s·∫£n ph·∫©m theo doanh thu:")
print(top10_revenue)

# V·∫Ω
top10_quantity.plot(kind="barh", figsize=(10,6), title="Top 10 s·∫£n ph·∫©m theo s·ªë l∆∞·ª£ng")
plt.xlabel("S·ªë l∆∞·ª£ng")
plt.show()

top10_revenue.plot(kind="barh", figsize=(10,6), title="Top 10 s·∫£n ph·∫©m theo doanh thu")
plt.xlabel("Doanh thu")
plt.show()


# Doanh thu theo qu·ªëc gia
revenue_by_country = df.groupby("Country")["Revenue"].sum().sort_values(ascending=False)

# Doanh thu theo gi·ªù trong ng√†y
revenue_by_hour = df.groupby(df["InvoiceDate"].dt.hour)["Revenue"].sum()

# Doanh thu theo ng√†y trong tu·∫ßn (0 = Monday, 6 = Sunday)
revenue_by_weekday = df.groupby(df["InvoiceDate"].dt.dayofweek)["Revenue"].sum()

print(" Doanh thu theo qu·ªëc gia (Top 10):")
print(revenue_by_country.head(10))

print("\n Doanh thu theo gi·ªù trong ng√†y:")
print(revenue_by_hour)

print("\n Doanh thu theo ng√†y trong tu·∫ßn (0=Mon, 6=Sun):")
print(revenue_by_weekday)

# V·∫Ω
revenue_by_country.head(10).plot(kind="bar", figsize=(12,6), title="Doanh thu theo qu·ªëc gia (Top 10)")
plt.ylabel("Doanh thu")
plt.show()

revenue_by_hour.plot(kind="bar", figsize=(10,6), title="Doanh thu theo gi·ªù trong ng√†y")
plt.ylabel("Doanh thu")
plt.show()

revenue_by_weekday.plot(kind="bar", figsize=(10,6), title="Doanh thu theo ng√†y trong tu·∫ßn")
plt.ylabel("Doanh thu")
plt.show()


basket_size = df.groupby("InvoiceNo")["Quantity"].sum()
print("Min:", basket_size.min())
print("Max:", basket_size.max())
print("Mean:", basket_size.mean())
print("Median:", basket_size.median())


# max: 15049 s·∫£n ph·∫©m tr√™n 1 gi·ªè h√†ng. D·ªØ li·ªáu b·ªã l·ªách
# median: 151 nh∆∞ng max cao d·∫´n ƒë·∫øn b·ªã sai s·ªë d·ªØ li·ªáu


from datetime import timedelta

# Ng√†y tham chi·∫øu (1 ng√†y sau ng√†y cu·ªëi trong data)
reference_date = df["InvoiceDate"].max() + timedelta(days=1)


# T√≠nh RFM cho t·ª´ng kh√°ch
rfm = df.groupby("CustomerID").agg({
    "InvoiceDate": lambda x: (reference_date - x.max()).days,  # Recency
    "InvoiceNo": "nunique",                                   # Frequency
    "Revenue": "sum"                                          # Monetary
}).reset_index()

rfm.rename(columns={"InvoiceDate": "Recency",
                    "InvoiceNo": "Frequency",
                    "Revenue": "Monetary"}, inplace=True)


from sklearn.preprocessing import StandardScaler
# N·∫øu c·ªôt Segment ch∆∞a t·ªìn t·∫°i, t·∫°o t·∫°m
if "Segment" not in rfm.columns:
    print("‚ö†Ô∏è C·ªôt 'Segment' kh√¥ng t·ªìn t·∫°i ‚Äî t·∫°o m·∫∑c ƒë·ªãnh 'ToCluster'")
    rfm["Segment"] = "ToCluster"

# Ch·ªâ l·∫•y kh√°ch h√†ng thu·ªôc nh√≥m c·∫ßn ph√¢n c·ª•m
# N·∫øu ch∆∞a c√≥ c·ªôt 'Segment', t·∫°o m·∫∑c ƒë·ªãnh
rfm_cluster = rfm.loc[
    rfm.get("Segment", pd.Series(["ToCluster"] * len(rfm))) == "ToCluster",
    ["Recency", "Frequency", "Monetary"]
]


# Chu·∫©n h√≥a d·ªØ li·ªáu tr∆∞·ªõc khi ph√¢n c·ª•m
scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(rfm_cluster)

# Trung b√¨nh RFM theo t·ª´ng c·ª•m
if "Cluster" in rfm.columns:
    cluster_profile = rfm.groupby("Cluster")[["Recency", "Frequency", "Monetary"]].mean()
    print("\nüìä Trung b√¨nh RFM m·ªói c·ª•m:")
    print(cluster_profile)
else:
    print("‚ö†Ô∏è Ch∆∞a c√≥ c·ªôt 'Cluster' ‚Äî b·ªè qua th·ªëng k√™ trung b√¨nh c·ª•m.")

# G√°n nh√£n d·ªÖ hi·ªÉu cho t·ª´ng c·ª•m
cluster_labels = {
    0: "Kh√°ch gi√° tr·ªã cao",
    1: "Kh√°ch trung th√†nh",
    2: "Kh√°ch m·ªõi",
    3: "Kh√°ch r·ªßi ro"
}

if "Cluster" in rfm.columns:
    rfm["Segment"] = rfm["Cluster"].map(cluster_labels).fillna(rfm["Segment"])

print("‚úÖ Ho√†n t·∫•t g√°n nh√£n ph√¢n kh√∫c kh√°ch h√†ng.")



from mlxtend.preprocessing import TransactionEncoder

# Gom s·∫£n ph·∫©m theo h√≥a ƒë∆°n
basket = df.groupby("InvoiceNo")["Description"].apply(list)

# List of list
transactions = basket.tolist()

# M√£ h√≥a one-hot
te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
df_basket = pd.DataFrame(te_ary, columns=te.columns_)


from datetime import timedelta

# Chia d·ªØ li·ªáu: 80% train, 20% test theo th·ªùi gian
split_date = df["InvoiceDate"].quantile(0.8)
df_train = df[df["InvoiceDate"] <= split_date].copy()
df_test  = df[df["InvoiceDate"] > split_date].copy()

# T·∫°o doanh thu
df_train["Revenue"] = df_train["Quantity"] * df_train["UnitPrice"]
df_test["Revenue"]  = df_test["Quantity"] * df_test["UnitPrice"]

# Reference date cho train
ref_date = df_train["InvoiceDate"].max() + timedelta(days=1)

# RFM t·ª´ train
rfm_train = df_train.groupby("CustomerID").agg({
    "InvoiceDate": lambda x: (ref_date - x.max()).days,  # Recency
    "InvoiceNo": "nunique",                              # Frequency
    "Revenue": "sum"                                     # Monetary
}).reset_index().rename(columns={"InvoiceDate":"Recency","InvoiceNo":"Frequency","Revenue":"Monetary"})

# Nh√£n = chi ti√™u ·ªü holdout
future_spend = df_test.groupby("CustomerID")["Revenue"].sum().reset_index().rename(columns={"Revenue":"Future_Monetary"})

# Merge
rfm = rfm_train.merge(future_spend, on="CustomerID", how="left").fillna(0)


rfm["Future_pred"] = model.predict(X)
rfm["CLV_pred_12m"] = rfm["Future_pred"]     # d·ª± ƒëo√°n cho 12 th√°ng
rfm["CLV_pred_6m"]  = rfm["Future_pred"]*0.5 # gi·∫£ ƒë·ªãnh 6 th√°ng = 1/2 nƒÉm


# In ra 5 kh√°ch h√†ng gi√° tr·ªã cao nh·∫•t

top5 = rfm.sort_values("CLV_pred_12m", ascending=False).head(5)
print(top5[["CustomerID","Recency","Frequency","Monetary","CLV_pred_6m","CLV_pred_12m"]])


import numpy as np

# Ng∆∞·ª°ng VIP = top 1% Monetary
vip_threshold = rfm["Monetary"].quantile(0.99)
rfm["Segment"] = np.where(rfm["Monetary"] > vip_threshold, "VIP", "ToCluster")


from mlxtend.frequent_patterns import fpgrowth, association_rules

# Frequent itemsets (min_support = 0.01 = 1%)
frequent_itemsets = fpgrowth(df_basket, min_support=0.01, use_colnames=True)

# Sinh lu·∫≠t k·∫øt h·ª£p
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)

# L·ªçc lu·∫≠t m·∫°nh (confidence >= 0.5, lift > 1)
rules_strong = rules[(rules["confidence"] >= 0.5) & (rules["lift"] > 1)]

# Top 10 lu·∫≠t
print(rules_strong.sort_values("lift", ascending=False).head(10))


